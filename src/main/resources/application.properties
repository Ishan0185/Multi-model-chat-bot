spring.application.name=Multi Model ChatClient

# =========================
# Ollama Configuration
# Ollama runs locally and therefore must be installed and configured on your machine,
# unlike cloud-based LLM services such as OpenAI.

# =========================
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=llama2
spring.ai.ollama.chat.options.temperature=0.7

# =========================
# OpenAI Configuration
# =========================
spring.ai.openai.api-key=${OPENAI_API_KEY}
spring.ai.openai.chat.model=gpt-4o-mini
spring.ai.openai.chat.options.temperature=0.7

# =========================
# Server Settings
# =========================
server.port=8080